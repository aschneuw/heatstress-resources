\chapter{Follow-Up Project Proposals}
\section{Enhancing Agronomic Understanding of Heat Stress in Swiss Dairy Cow Breeds}
\paragraph{Project Description} The work and results established in Chapters~\ref{chap:introduction}-\ref{chap:results} examine the average impact of temperature and humidity on the dairy productivity of various Swiss cow breeds with Generalized Additive Mixed Models (GAMMs). Specifically, we compute the non-linear marginal effect of the Temperature Humidity Index (THI) on six dairy cow breeds and identify the THI turning points to delineate peak performance relative to weather conditions. Additionally, the model and methodology provide the national average long-term lactation curve for each breed, as well as the associated lactation peak-points. Detailed limitations of the current work are listed in Section~\ref{sec:limitations}. The structure of the dataset and computational limitations leave open questions regarding the model's correctness, validity of confidence intervals because of spatial autocorrelation, and the selection of an appropriate subsampling strategy, which must account for a broad geospatial and THI diversity as well as for different farm structures. This follow-up project aims to validate and refine the model using the established data and methodologies in Chapter~\ref{chap:material_methods}. The work includes conducting a thorough investigation of theoretical justifications for subsampling strategies as well as model selection, and systematically applying them to a comprehensive dataset comprising 130 million test-day milk samples collected from over 4.2 million dairy cows across almost 47'000 commercial dairy farms in Switzerland over the past four decades.

\paragraph{Field} Agricultural Science, Statistics, Data Science
\paragraph{Scope} 2 or more subprojects
\paragraph{Details} To reinforce the findings presented in Chapter~\ref{chap:results}, it is imperative to address the following points: First, a comprehensive analysis and justification of the correctness of single-breed models (Equation~\ref{eq:single_breed_model_extended}) relative to multi-breed models (Equation~\ref{eq:multi_breed_model_extended}) must be established. Second, this analysis may necessitate an exploration of alternative data subsampling strategies, given the computational constraints associated with the current methodology. The current subsampling strategy is elaborated upon in Chapter~\ref{chap:results}, and the various subsamples are summarized in Section~\ref{sec:model_overview}. Third, the existing model does not differentiate between dairy cows with low and high milk production levels. The low THI turning points discussed in Chapter~\ref{chap:results} and the diagnostic plots presented in the Appendix~\ref{appendix:models} indicate that this production-level distinction merits further investigation. In addition, further analyses include farm locations as fixed effects or integrating additional meteorological variables as control variables. Familiarity with statistical modelling and readiness to operate with R, Python and Julia are essential.

\paragraph{Keywords} Heat Stress, Dairy Performance, Breeds, High- vs Low-producing cows, Lactation curves, Spatial Autocorrelation, Subsampling Strategies, Big Data, Heterogeneous Treatment Effects, GAMs

\newpage

\section{GAMMs for Large but Sparse Random Effect Structures I}
\paragraph{Project Description} \textit{mgcv} by \cite{wood_generalized_2017} is an R library which implements various methods around the estimation of Generalized Additive Models (GAMs) and Generalized Additive Mixed Models (GAMMs). An extension provided by \cite{wood_gamm4_2020} through \textit{gamm4} facilitates the estimation of GAMMs subject to a large number of random effects. The conversion of a GAMM into a mixed model is addressed by \textit{gamm4} \citep{wood_stable_2004}. Despite these advancements, \textit{gamm4} encounters computational bottlenecks. In response, we have developed a prototype, denoted as \textit{gamm4b} (Section~\ref{sec:gamm4b}), which alleviates some of these computational challenges. The primary aim of this project is to engage in dialogues with the authors of \textit{gamm4} regarding our modifications, refine the underlying code, ensure compatibility of these modifications with all supported input models as identified in \textit{gamm4}, develop integration tests utilizing diverse datasets, evaluate the performance, and release the updated library as open-source. This project serves to address the existing gap in the accurate estimation of GAMMs with tens of thousands of random effect factor levels. The development and testing phases are supported by a comprehensive dataset comprising 130~million test-day milk samples collected from over 4.2~million dairy cows across almost 47'000 commercial dairy farms in Switzerland over the past four decades.

\paragraph{Field} Computer Science, Data Science, Statistics

\paragraph{Scope} 1 project

\paragraph{Details} GAMs can be conceptualized as a subclass of mixed models. The equivalence, as delineated by \cite{wood_stable_2004}, is elaborated upon in Section~\ref{sec:mixed_to_gams}. This dichotomy proves advantageous when dealing with extensive yet sparse random effect structures. For instance, this is particularly relevant in models that integrate unobserved heterogeneity such as random effects within our dataset, which consists of milk samples sourced from thousands of farms and encompassing millions of cows (Section~\ref{sec:agronomic_data}). While \textit{gamm4} is specifically designed to manage a large number of random effects, its performance declines when confronted with the substantial volume of samples inherent in the aforementioned dataset. In Section~\ref{sec:gamm4b}, we present a prototype intended to enhance \textit{gamm4}. Currently, this prototype is limited to addressing the Gaussian instance of mixed models. The enhancements discussed necessitate rigorous validation, optimization, and expansion to the generalized, non-Gaussian variant of mixed models, thereby accommodating all model types endorsed by the default \textit{gamm4} interface. Furthermore, it is anticipated that comprehensive integration testing and performance assessments across varying dataset sizes will be developed. The code of our prototype is written in R and Python. As needed, certain components may require refactoring in C++ utilizing \textit{Rcpp} from \cite{rcpp}, to bolster either usability or performance. The developer will gain proficiency in R matrix libraries, GAMs, GAMMs, mixed models, sparse matrix computations, as well as Cholesky decompositions.

\paragraph{Keywords} R, Python, C++, Sparse Matrix Algebra, Choleksy Decomposition, GAMMs, gamm4, Big~Data, Performance Evaluation, Testing, Open Source

\newpage

\section{Scalable Data Covariance Matrix Computation for GAMM Inference}
\paragraph{Project Description} In the process of estimating Generalized Additive Mixed Models (GAMMs) as mixed models \citep{wood_stable_2004}, the inference phase necessitates the Cholesky decomposition of a data covariance matrix $\bm{V}$ \citep[page 289]{wood_generalized_2017}. The data covariance matrix increases linearly with the number of random effect factor levels and quadratically with the number of samples, thus posing a significant computational and memory resource bottleneck when dealing with large datasets. Sections \ref{sec:gamm4b} and \ref{sec:cholesky_scalable} in our work highlight this challenge. The objective of this project is to investigate the potential methods for simplifying, approximating, or alternatively distributing the computation of the data covariance matrix $\bm{V}$ Cholesky decomposition across a high-performance distributed system with multiple nodes. This endeavor aspires to address an existing gap in the estimation of GAMMs, particularly when the random effect factor levels may escalate to the magnitude of millions. The development and testing phases are supported by a comprehensive dataset comprising 130~million test-day milk samples collected from over 4.2~million dairy cows across almost 47'000 commercial dairy farms in Switzerland over the past four decades.

\paragraph{Field} Statistics, Computational Science, Data Science

\paragraph{Scope} 1 project

\paragraph{Details} \cite{wood_generalized_2017} have enhanced their standard routines \textit{bam} and \textit{gam} to manage datasets consisting of millions of samples with a limited number of fixed effects. This results in model design matrices that are elongated yet narrow, characterized by a large number of rows and relatively few columns. However, when addressing design matrices derived from models and large but sparse datasets akin to ours (Section~\ref{chap:material_methods}, Figure~\ref{fig:dataset_structure_sparsity_dynamics}, Figure~\ref{fig:dataset_sample_distribution}), which exhibit random effect structures encompassing up to millions of factor levels, these matrices become not only elongated but also broad, whereupon the algorithms in question exhibit limitations. These algorithms are optimized for depth but are constrained in terms of breadth. \textit{gamm4} offers a partial solution to this challenge, yet as far as our understanding extends, \cite{wood_generalized_2017} recognizes the computational intensity associated with operations related to the data covariance matrix. We mitigate these bottlenecks through a prototype implementation utilizing high-performance sparse matrix libraries in Julia (Section~\ref{sec:cholesky_scalable}). However, these measures are inadequate for handling millions of samples within the context of our models proposed in Chapter~\ref{chap:material_methods}. Consequently, it is imperative to evaluate whether and how operations pertaining to \textit{gamm4} inference can be optimized, or, alternatively, to determine what measures are necessary to facilitate the scaling of operations within a scalable multi-node cluster. This project necessitates a profound comprehension of both the mathematical and technical aspects of GAMMs with the aim of enabling scalable estimation of such models at an unprecedented scale of random effect factor levels.

\paragraph{Keywords} Random Effects, Mixed Models, GAMMs, mgcv, gamm4, Sparse Matrix Algebra, Cholesky Decomposition, Data Covariance Matrix, Scalability

\newpage

\section{GAMMs for Large but Sparse Random Effect Structures II}
\paragraph{Project Description} \textit{mgcv} by \cite{wood_generalized_2017} is an R library designed to implement various methodologies for the estimation of Generalized Additive Models (GAMs) and Generalized Additive Mixed Models (GAMMs). An extension provided by \cite{wood_gamm4_2020} through \textit{gamm4} facilitates the estimation of GAMMs that involve a large number of random effects. This routine transforms a GAMM into a mixed model \citep{wood_stable_2004}. Despite these advancements, \textit{gamm4} encounters computational bottlenecks. In response, we have developed a prototype in Julia, referred to as \textit{gammJ} (Section~\ref{sec:gammJ}), which mitigates some of these computational challenges by employing a modified version of \textit{MixedModels.jl} as the underlying fitting mechanism and optimizing data-covariance matrix computations (Section~\ref{sec:cholesky_scalable}). The current prototype executes the model-building and model inference steps for analysis and diagnostics within the R environment. These two steps are also referred to as pre- and post-processing procedures. The model estimation process, this is the step between the pre- and post-processing procedures, is conducted in our modified Julia engine as opposed to \textit{lme4} in the conventional \textit{gamm4}. The prototype is implemented in an R-based Jupyter notebook that manages the communication between the distinct programming environments. The principal objective of this project is to refine the underlying code, ensure the compatibility of these modifications with all supported input models as identified in \textit{gamm4}, develop integration tests utilizing diverse datasets, evaluate performance, and ultimately release an open-source library. This project aims to bridge the existing gap in the precise estimation of GAMMs, integrating hundreds of thousands, or even millions, of random effect factor levels. The development and testing phases are underpinned by an extensive dataset comprising 130 million test-day milk samples collected from over 4.2 million dairy cows spanning nearly 47,000 commercial dairy farms in Switzerland over the past four decades.

\paragraph{Field} Computer Science, Data Science, Computational Science, Statistics

\paragraph{Scope} 1-2 subprojects

\paragraph{Details} The library \textit{MixedModels.jl} represents a faster alternative to \textit{lme4} for fitting mixed models. Nonetheless, the standard implementation of \textit{MixedModels.jl} does not accommodate mixed models produced through a GAMM to mixed model conversion. Section~\ref{sec:julia_modifications} details the software modifications applied to enable the estimation of this category of models. First, it is imperative to engage with the authors of \textit{MixedModels.jl} to ascertain whether these modifications could be integrated into their library or necessitate the publication of an independent package. Second, the current state of some sparse matrix operations does not fully leverage parallelization for scaling with increased CPU core counts on a single machine. This necessitates a meticulous performance evaluation of the computational steps to identify and eliminate single-threaded bottlenecks. Third, it is essential that the novel random effect matrix type and its associated operations, introduced by our modifications, also support non-Gaussian distributions. Fifth, the existing \textit{gammJ} prototype interfaces with the modified Julia library via external application calls from R, producing temporary commands, temporary input, and temporary output files. This strategy serves as a workaround to the constraints posed by the immature in-memory bridges linking Julia to R. Nevertheless, it is crucial to consider whether integrating our contributions directly into \textit{GAM.jl} might be worthwhile. This effort was initially started by \cite{Henderson_GAMjl} to enable \textit{mgcv} functionality into Julia. A probable conclusion from this examination is that the current methodology—maintaining model-definition, plotting, and model diagnostics functionalities within R—constitutes the most practical approach. A comprehensive implementation of \textit{gamm4} entirely in Julia may prove excessively demanding. Moreover, incorporating all the changes into \textit{gamm4} would deviate excessively from its primary objectives. Consequently, the development of a new R package that utilizes a hybrid approach involving both R and Julia seems to be the most viable solution. Fifth, the package is intended to encompass as much of \textit{gamm4}'s functionality as feasible. Lastly, the creation of tests utilizing diverse datasets and a comparative performance evaluation against \textit{gamm4} are critical to underscore the advantages of the new package. This software development project necessitates concurrent work in both R and Julia, with an aim towards optimizing the library for maximal scaling efficiency within a shared-memory, single-node environment.

\paragraph{Keywords} GAMMs, Julia, R, Mixed Models, Cholesky Decomposition, Sparse Matrix Algebra, Package Development, Performance Evaluation

\newpage

\section{Spatial Autocorrelation for Gaussian Mixed Models}
\paragraph{Project Description} The presence of spatial autocorrelation among samples contravenes the assumption of independent and identically distributed (iid) residuals. This issue is also pertinent in our study, which examines the impact of heat stress on Swiss dairy cow breeds (Chapters~\ref{chap:introduction}-\ref{chap:results}). In Generalized Additive Mixed Models (GAMMs), this challenge can be addressed either by incorporating spatial autocorrelation into the systemic component \citep{dupont_spatial_2020} of the model or by relaxing the iid assumption to estimate autocorrelation structures \citep{StackExchange_GAM_Spatial_Autocorrelation} via mixed models using the library \textit{nlme}. Nonetheless, the existing \textit{nlme} implementation demonstrates suboptimal performance and is inadequate for handling large datasets. This project aims to determine the feasibility of efficiently implementing residual autocorrelation structures within Gaussian Mixed Models using the library \textit{MixedModels.jl}. Should the assessment yield positive results, a reference implementation will be developed. Consequently, this project seeks to bridge the existing gap in estimating mixed models with spatially correlated residuals for large datasets. The development and testing procedures are supported by an extensive dataset comprising 130 million test-day milk samples collected from over 4.2~million dairy cows across nearly 47,000~commercial dairy farms in Switzerland over the past four decades.

\paragraph{Field} Statistics, Computer Science, Data Science
\paragraph{Scope} 1 project
\paragraph{Details} In reference to \cite{bolker_glmmFAQ}, the task of implementing residual autocorrelation structures in \textit{lme4} is described as "tedious but straightforward". This assumption is anticipated to be applicable to \textit{MixedModels.jl}. The developer is required to get familiar with the intricate details of mixed models \cite{bates_fitting_2015} as well as the fundamental concepts of spatial autocorrelation \citep{cressie1991statistics}. The objective is to conduct a comprehensive feasibility analysis, leading to a high-performance implementation in Julia, contingent upon a favorable outcome from the analysis.

\paragraph{Keywords} GAMMs, Spatial Autocorrelation, Julia, Mixed Models

\newpage


\section{Hourly THI Aggregation with Broken-Stick Models and GAMs}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{thesis/figures/thi_load.png}
    \caption[]{Interpretable THI feature engineering with THI hours and THI exposure. The figure depicts the hourly THI values for three days starting at reference day $t$ while the available data points are the daily minimum and maximum THI values. The hourly values are interpolated with a double since curve \citep{bucheli_heat_2022}. The dark green line marks a THI cut-off value also referred as the THI exposure. Summing all THI hours below and above this cutoff value, depicted by the light green and pink frames, results in the aggregated THI hours below and above the selected cut-off value. The concept can be extended to multiple cutoff-values.}
    \label{fig:thi_load}
\end{figure}
\paragraph{Project Description} Smooth terms in Generalized Additive Models (GAMs) offer the advantage of estimating non-linear marginal effects of a covariate on a dependent variable with minimal preliminary assumptions regarding non-linearity. For instance, in assessing the impact of the Temperature Humidity Index (THI) on dairy performance across Swiss dariy cow breeds, smooth terms facilitate a straightforward and intuitive estimation of this effect, as demonstrated in our research detailed in Chapters~\ref{chap:introduction}-\ref{chap:results}. Our methodology presented in Chapter~\ref{chap:material_methods} applies the three-day mean THI as a covariate. A potential limitation of this methodology is that averaging values possibly obscures intra-daily fluctuations in the THI. However, these within-day fluctuations might already suffice for cows to recover from heat stress. Therefore, it is worth considering the duration of exposure of cows to specific THI levels \citep{kadzere_heat_2002}. Consequently, evaluating the effect of hourly exposure at specified THI levels could be more intuitive. Informed by \citet{schlenker_nonlinear_2009}, \cite{bucheli_heat_2022} and \cite{vroege_effects_2023} develop the concept of THI load: for predefined buckets they sum THI hours. Let us consider the example with two buckets which capture THI hours from $[0;55]$ and $]55;100]$. Hence, 55 is a predefined cut-off value. To simplify, we capture and aggregate THI of only for a time period of four hours: The first hour has THI 51, the second 53, the third 58 and the last 50. The first bucket is filled with $51+53+55+50=209$ THI hours. The second bucket is filled with $3$ THI hours. This means that the first bucket has a THI exposure of 209 THI hours and the second bucket a THI exposure of 3 THI hours. Hence, with one or more THI knot points the number of buckets as well as their width is determined and then they are filled up for each sample with the associated THI hours. One aggregates all THI hours for values up to the cut-off and another for hours beyond the cut-off. In our example with two buckets, this process results in two covariates: Subsequently, a piecewise-linear regression\footnote{broken stick} is applied. This process is repeated for a predefined set of THI cut-off points to iteratively search for the model with the most suitable cut-off point. The model with the lowest Akaike information criterion (AIC) is considered as the most representative fit. The bucket coefficients represent marginal effect of the cumulative hourly THI exposure below and above the cut-off THI value. By substracting the mean marginal effect, a plot can be centered to illustrate the marginal effect of an additional hourly exposure. An advantage of this method is the flexibility it offers in selecting the aggregation time window for hourly THI data, allowing it to be tailored according to the availability of data while keeping the interpretation of hourly exposure. In their analysis, \cite{vroege_effects_2023} and \cite{bucheli_heat_2022} utilizes monthly (dependent variable summed over a month) or yearly (dependent variable summed over a year) farm-level data. This methodology allows to find the optimal cut-off value enables the identification of turning points similarly to our approach. However, the piecewise-linear assumption posits that the effect of THI is linear before and after the THI turning points. Although this method offers a clear and very powerful interpretability through the concept of THI hours, the assumption of linearity may be overly restrictive: Firstly, in our experiments (Chapter~\ref{chap:results} and Appendix~\ref{appendix:models}) as well as in comparable research \citep{vinet_estimation_2023}, within a three-day average THI range of 30-70, an inverse parabolic behavior is observed. Secondly, at colder temperatures below THI values of 30, highly non-linear behaviors are evident, albeit with a limited number of samples. Considering these observations, employing broken stick approaches to evaluate heat stress effects during weather periods above 40 three-day THI points to determine THI turning points seems to be a justified approximation since a inverse parabola can be approximated fairly well with one single cut-off value. Nonetheless, the marginal effects may be moderately biased since they are not entireley linear. Therefore, with data spanning multiple years and geospatially diverse areas, a single-knot broken stick approach may be overly simplistic, as the estimated THI covariate coefficients below the turning point would be significantly skewed by the non-linear responses below 40 THI points. Using multiple knots can lead to better approximations. However, a grid-search is an exhaustive parameter search. The introduction of multiple knots may render the search space computationally impractical when employing a broken-stick multi-breed mixed model to identify the optimal configuration of cut-off values.

Thus, we remain with the open question if aggregated THI hours can be meaningfully integrated within the GAM framework, as opposed to employing a broken stick approach. Merging the concept of aggregated THI hours with non-linear smooth terms would retain interpretability while more effectively addressing non-linearity. The goal is to have a single smooth term that assesses the marginal effect of THI, where the abscissa represents the hourly THI exposure and the ordinate signifies the marginal effect on the dependent variable over the aggregated time period. Internally, smooth terms inherently create knot points, albeit through varying basis functions. Consequently, this project aims to ascertain whether and how the concept of cumulative THI hours and THI exposure may be effectively integrated into smooth terms within the GAM framework. This requires a thorough understanding of the different types of smooth terms in GAMs \cite[page 195-248]{wood_generalized_2017}. This project may necessitate the formulation of a novel and for this use case tailored smooth term. We seek to address this gap concerning the interpretability of aggregated THI values while leveraging the capacity for non-linearity offered by smooth terms and maximum likelihood estimation strategies within GAMMs. The development and testing phases are underpinned by an extensive dataset comprising 130 million test-day milk samples collected from over 4.2 million dairy cows spanning nearly 47,000 commercial dairy farms in Switzerland over the past four decades.

\paragraph{Field} Agricultutral Science, Statistics, Data Science

\paragraph{Scope} 1-2 subprojects

\paragraph{Details} Initially, a reference broken-stick mixed model applying the concept of THI hours depicted in Figure~\ref{fig:thi_load}, inspired by the single-breed model in Equation~\ref{eq:single_breed_model} or multi-breed model delineated in Equation~\ref{eq:multi_breed_model_extended}, will be defined and estimated with \textit{MixedModels.jl}. The search for optimal cut-off values will be carried out using a THI grid search as in \cite{bucheli_heat_2022}. It is essential to address constraints pertaining to the data and annual time-spans to ensure validity and computational feasibility. The results establish a baseline model for comparison with \cite{vroege_effects_2023}. The major difference between \cite{bucheli_heat_2022} as well as \cite{vroege_effects_2023} is that in our case we work with mixed models due to the hierarchical and non-regular structure of our data (Section~\ref{sec:agronomic_data}). Subsequently, a meticulous mathematical evaluation is undertaken to ascertain the feasibility of incorporating the concept of THI hours and THI exposure within the GAM framework. In case of a positive outcome, the methodology will be demonstrated using either single or multi-breed models to quantify the non-linear impact of THI hours on dairy performance at the cow level. Willingness to get familiar with the detailed mathematical concepts of smooth terms is required for a successful execution of this project.

\paragraph{Keywords} Heat Stress, THI Hours, THI Exposure, GAMs, Smooth Terms, Basis Functions, Splines, Aggregation